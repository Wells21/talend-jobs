# Snowflake to Salesforce Integration Pipeline (Talend)

## üìå Project Overview

This project implements an end-to-end ETL pipeline using **Talend** to:

1. Extract structured data from a **Snowflake** data warehouse  
2. Transform and align the schema to match **Salesforce API object fields**  
3. Load the transformed data into **Salesforce** using the Bulk API  

The solution is designed for high-volume ingestion, schema consistency, error handling, and production stability.

---

## üèóÔ∏è Architecture Overview

**Source** ‚Üí Snowflake  
**Transformation Layer** ‚Üí Talend (tDBInput, tMap, validation, logging)  
**Target** ‚Üí Salesforce (Bulk API)

### High-Level Flow

Snowflake ‚Üí Talend Extraction ‚Üí Schema Mapping (tMap)
‚Üí Salesforce Bulk Output ‚Üí Bulk Execution
‚Üí Success / Reject Handling ‚Üí Logging


---

## ‚öôÔ∏è Technologies Used

- Talend Open Studio / Talend Data Integration  
- Snowflake  
- Salesforce Bulk API  
- SQL  
- API-based Data Integration  
- Logging & Reject Handling Strategy  

---

## üîÑ Workflow Breakdown

### 1Ô∏è‚É£ Pre-Job Initialization

Talend initializes required connections and security configurations.

**Components Used:**

- `tPrejob`  
- `tSetTalendKeyStore`  
- Snowflake connection component  
- `tSalesforceConnection`  

**Responsibilities:**

- Initialize context variables  
- Establish secure Snowflake connection  
- Establish Salesforce API session  
- Ensure job execution order control  

---

### 2Ô∏è‚É£ Data Extraction (Snowflake)

**Component:** `tDBInput`

**Responsibilities:**

- Execute SQL query against Snowflake  
- Retrieve required dataset  
- Enforce explicit column selection  
- Apply optional filtering  

**Example Query (Conceptual):**

```sql
SELECT 
    building_id,
    building_name,
    city,
    state,
    created_date
FROM analytics.buildings;
```

3Ô∏è‚É£ Data Transformation & Field Mapping

Component: tMap

This layer aligns the Snowflake schema with the Salesforce object structure.

Responsibilities:

1. Mapped Snowflake fields to Salesforce object fields

2. Performed data type alignment

3. Handled null values

4. Applied conditional logic

5. Formatted date/time fields

6. Applied default values where required

7. Applied Picklist logic on some field, making sure whatever data coming in must map to 5 pickilist vlaues if not reject.
This ensures data quality on the salesforce object.

Example Field Mapping;

| Snowflake Column | Salesforce Field |
| ---------------- | ---------------- |
| building_id      | External_Id__c   |
| building_name    | Name             |
| city             | mayestro__City__c          |
| state            | mayestro__State__c         |
| created_date     | mayestro__Created_Date__c  |

This explicit mapping prevents schema drift and runtime failures.

4Ô∏è‚É£ Bulk Data Load (Salesforce)

Components Used:

* tSalesforceOutputBulk

* tSalesforceBulkExec

Load Strategy:

* Bulk API mode

* Batch processing enabled

* Insert / Upsert based on use case

* External ID-based deduplication

Why Bulk API?

* Optimized for high-volume records

* Asynchronous processing

* Reduced API call overhead

* Better performance vs. REST mode

5Ô∏è‚É£ Error Handling & Logging

The job implements structured error handling to prevent silent failures.

Mechanisms Used:

* Reject flow (Reject link)

* tLogRow for debugging (removed during production)

* Optional file output for failed records

Flow Structure:

* Main Flow ‚Üí Successfully processed records

* Reject Flow ‚Üí Failed records captured separately

This ensures traceability and resilience.


üìà Business Impact

Automated manual data synchronization

Reduced integration errors

Improved data consistency between Snowflake and Salesforce

Scalable architecture for future growth

Production-ready ETL framework


üë§ Author

Wellspring
Data Engineer | Talend ETL Developer | SQL & API Integration Specialist
